import requests
from bs4 import BeautifulSoup
from db import insert_news, news_exists
import time
from urllib.parse import urljoin
from deep_translator import GoogleTranslator
import re

# --------------------------
# åˆå§‹åŒ– Cohere æ”¹å†™ API
# --------------------------
OPENAI_API_KEY = "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"
COHERE_URL = "https://api.cohere.ai/v1/chat"

def rewrite_text_chatgpt(text: str):
    """
    æ”¹å†™æ–‡æœ¬ï¼Œè‡ªåŠ¨å¤„ç†é•¿æ–‡æœ¬åˆ†æ®µï¼Œå¤±è´¥è‡ªåŠ¨é‡è¯•
    """
    chunks = [text[i:i+MAX_TOKENS_PER_REQUEST] for i in range(0, len(text), MAX_TOKENS_PER_REQUEST)]
    rewritten = ""

    for chunk in chunks:
        for attempt in range(3):  # æœ€å¤šé‡è¯• 3 æ¬¡
            try:
                url = "https://api.openai.com/v1/chat/completions"
                headers = {
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                }
                payload = {
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸­æ–‡å†™ä½œåŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": f"è¯·ç”¨ä¸­æ–‡æ”¹å†™ä»¥ä¸‹æ–‡æœ¬ï¼Œä¿æŒåŸæ„ä½†ç”¨ä¸åŒæªè¾ï¼š\n\n{chunk}"}
                    ],
                    "temperature": 0.7
                }
                resp = requests.post(url, headers=headers, json=payload, timeout=20)
                if resp.status_code != 200:
                    print(f"ChatGPT è¯·æ±‚å¤±è´¥ {resp.status_code}ï¼Œé‡è¯• {attempt+1}/3")
                    time.sleep(2)
                    continue
                data = resp.json()
                rewritten += data["choices"][0]["message"]["content"].strip() + "\n"
                break
            except Exception as e:
                print(f"å¼‚å¸¸ {e}ï¼Œé‡è¯• {attempt+1}/3")
                time.sleep(2)
        else:
            print("âš ï¸ æ”¹å†™å¤±è´¥ï¼Œä¿ç•™åŸæ–‡æœ¬")
            rewritten += chunk + "\n"
    return rewritten.strip()

# --------------------------
# åå¤„ç†ï¼šæ·»åŠ æ¢è¡Œï¼Œæ¯3å¥æ¢ä¸€æ¬¡è¡Œ
# --------------------------
def add_linebreaks(text, n_sentences=3):
    import re
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])', text)
    lines = []
    for i in range(0, len(sentences), n_sentences):
        lines.append("".join(sentences[i:i+n_sentences]))
    return "\n\n".join(lines)

# --------------------------
# ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡
# --------------------------
def translate_to_simplified(text: str) -> str:
    try:
        return GoogleTranslator(source="auto", target="zh-CN").translate(text)
    except Exception as e:
        print("ç¿»è¯‘å¤±è´¥:", e)
        return text

def rewrite_text(text):
    ok, rewritten = rewrite_text_chatgpt(text)
    if not ok:  # âŒ Cohere å¤±è´¥
        return None

    rewritten = add_linebreaks(rewritten)
    rewritten = translate_to_simplified(rewritten)
    time.sleep(61)  # âœ… æ¯æ¬¡è°ƒç”¨åå¼ºåˆ¶ç­‰å¾… 61 ç§’
    return rewritten


# --------------------------
# ä»¥ä¸‹æŠ“å–æ–‡ç« å†…å®¹ã€å›¾ç‰‡ã€ç½‘ç«™æ–°é—»ç­‰ä¿æŒä¸å˜
# --------------------------
def fetch_article_content(link):
    if not link:
        return ""
    try:
        resp = requests.get(link, timeout=15)
        soup = BeautifulSoup(resp.text, "html.parser")

        if "udn.com" in link:
           div = (
                soup.select_one("section.article-content__editor")
                or soup.select_one("div.article-content__editor")
                or soup.select_one("div#article_body")
                or soup.select_one("div#story_body_content")  # å…œåº•æ—§ç‰ˆ
            )
        elif "ltn.com" in link:
            div = (
                soup.select_one("div.text")
                or soup.select_one("div.cont")
                or soup.select_one("div#newsContent")
            )
        elif "yahoo.com" in link:
            div = soup.select_one("article")
        else:
            div = None

        if div:
            paragraphs = div.find_all("p")
            content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
            return content
    except Exception as e:
        print(f"æŠ“æ–‡ç« å†…å®¹å¤±è´¥ ({link}): {e}")
    return ""

def fetch_article_image(link):
    if not link:
        return None
    try:
        resp = requests.get(link, timeout=15)
        soup = BeautifulSoup(resp.text, "html.parser")
        img_url = None

        if "udn.com" in link:
             # âœ… å…ˆæŠ“ og:image / twitter:image
            meta = soup.select_one('meta[property="og:image"]') or soup.select_one('meta[name="twitter:image"]')
            if meta:
                img_url = meta.get("content")

            # å¦‚æœ meta æ²¡æœ‰ï¼Œå†é€€å›æ­£æ–‡
            if not img_url:
                div = (
                    soup.select_one("div#story_body_content")
                    or soup.select_one("section.article-content__editor")
                )
                if div:
                    img = div.find("img")
                    if img:
                        img_url = img.get("data-src") or img.get("src")
           
        elif "ltn.com" in link:
            div = soup.select_one("div.text")
            if div:
                img = div.find("img")
                if img:
                    img_url = img.get("src")
        elif "yahoo.com" in link:
            meta = soup.select_one('meta[property="og:image"]')
            if meta:
                img_url = meta.get("content")

        if img_url and img_url.startswith("/"):
            img_url = urljoin(link, img_url)

        return img_url
    except Exception as e:
        print(f"æŠ“æ–‡ç« å›¾ç‰‡å¤±è´¥ ({link}): {e}")
    return None

def fetch_site_news(url, limit=20):
    news_items = []
    try:
        resp = requests.get(url, timeout=20)
        soup = BeautifulSoup(resp.text, "html.parser")

        if "udn.com" in url:
            items = soup.select("div.story-list__text a")
        elif "ltn.com" in url:
            # LTN é¦–é¡µå’Œ breakingnews é¡µç”¨è¿™ä¸ª selector å°±èƒ½æŠ“åˆ°æ–°é—»é“¾æ¥
            items = soup.select("ul.list a")
        elif "yahoo.com" in url:
            items = soup.select("a[href*='/news/']")
        else:
            items = []

        for item in items[:limit]:
            title = item.get_text(strip=True)
            link = item.get("href")
            if link and link.startswith("/"):
                link = urljoin(url, link)

            # âœ… é’ˆå¯¹ Yahooï¼šè¿›å…¥è¯¦æƒ…é¡µæŠ“çœŸæ­£æ ‡é¢˜
            if "yahoo.com" in url and link and link.startswith("http"):
                try:
                    article_res = requests.get(link, timeout=10)
                    article_soup = BeautifulSoup(article_res.text, "html.parser")

                    h1 = article_soup.select_one("h1")
                    if h1:
                        title = h1.get_text(strip=True)
                    else:
                        og_title = article_soup.select_one("meta[property='og:title']")
                        if og_title and og_title.get("content"):
                            title = og_title["content"].strip()
                        elif article_soup.title:
                            title = article_soup.title.string.strip()

                except Exception as e:
                    print(f"Yahoo æŠ“æ–‡ç« æ ‡é¢˜å¤±è´¥: {e}")

            # âœ… é’ˆå¯¹ UDNï¼šè¿›å…¥è¯¦æƒ…é¡µæŠ“çœŸæ­£æ ‡é¢˜
            elif "udn.com" in url and link and link.startswith("http"):
                try:
                    article_res = requests.get(link, timeout=10)
                    article_soup = BeautifulSoup(article_res.text, "html.parser")

                    h1 = article_soup.select_one("h1")
                    if h1:
                        title = h1.get_text(strip=True)
                    else:
                        og_title = article_soup.select_one("meta[property='og:title']")
                        if og_title and og_title.get("content"):
                            title = og_title["content"].strip()
                        elif article_soup.title:
                            title = article_soup.title.string.strip()

                except Exception as e:
                    print(f"UDN æŠ“æ–‡ç« æ ‡é¢˜å¤±è´¥: {e}")

            news_items.append((title, link))

    except Exception as e:
        print(f"æŠ“ {url} å‡ºé”™: {e}")
    return news_items

def fetch_news():
    all_news = []
    sites = [
        "https://udn.com/news/index",
        "https://news.ltn.com.tw/list/breakingnews",
        "https://tw.news.yahoo.com/"
    ]

    for url in sites:
        print(f"\nğŸŒ æ­£åœ¨æŠ“å–ç«™ç‚¹: {url}")
        site_news = fetch_site_news(url, limit=20)

        if not site_news:
            print(f"âš ï¸ {url} æ²¡æœ‰æŠ“åˆ°æ–°é—»")
            continue

        for title, link in site_news:
            if not link:
                print(f"âŒ è·³è¿‡ï¼šæ ‡é¢˜ [{title}] æ²¡æœ‰é“¾æ¥")
                continue
            if news_exists(link):
                print(f"â© å·²å­˜åœ¨: {link}")
                continue
            if not title or title.strip() == "":
                print(f"âŒ è·³è¿‡ï¼šç©ºæ ‡é¢˜ (link={link})")
                continue

            content = fetch_article_content(link)
            if not content:
                print(f"âŒ è·³è¿‡ï¼š[{title}] æ²¡æœ‰æ­£æ–‡å†…å®¹")
                continue

            image_url = fetch_article_image(link)

            try:
                # ---------- æ”¹å†™æ ‡é¢˜ ----------
                title_rw = rewrite_text(title)
                if not title_rw or title_rw.strip() == "":
                    print(f"âŒ æ”¹å†™æ ‡é¢˜å¤±è´¥ï¼Œè·³è¿‡ (link={link})")
                    continue

                # ---------- æ”¹å†™æ­£æ–‡ ----------
                content_rw = rewrite_text(content)
                if not content_rw or content_rw.strip() == "":
                    print(f"âŒ æ”¹å†™æ­£æ–‡å¤±è´¥ï¼Œè·³è¿‡ (link={link})")
                    continue

                # ---------- åå¤„ç† ----------
                title_rw = remove_comma_after_punct(title_rw)
                title_rw = dedup_sentences(title_rw)
                content_rw = remove_comma_after_punct(content_rw)
                content_rw = dedup_sentences(content_rw)

                # ---------- å…¥åº“ ----------
                insert_news(title_rw, content_rw, link, image_url)

                all_news.append({
                    "title": title_rw,
                    "content": content_rw,
                    "link": link,
                    "image_url": image_url
                })

                print(f"âœ… æˆåŠŸ: {title_rw[:30]}... (link={link})")

            except Exception as e:
                print(f"âŒ æ’å…¥å¤±è´¥: {title[:30]}... é”™è¯¯: {e}")

    print(f"\nğŸ“Š æœ¬æ¬¡å…±æˆåŠŸä¿å­˜ {len(all_news)} æ¡æ–°é—»")
    return all_news

def remove_comma_after_punct(title: str) -> str:
    # æ›¿æ¢å¥å·ã€æ„Ÿå¹å·ã€é—®å·åé¢ç´§è·Ÿçš„ä¸­è‹±æ–‡é€—å·
    title = re.sub(r'([ã€‚ï¼ï¼Ÿ])[,ï¼Œ]+', r'\1', title)
    return title

def dedup_sentences(text: str) -> str:
    # åˆ†å¥ï¼Œä¿ç•™æ ‡ç‚¹
    parts = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
    sentences = []
    for i in range(0, len(parts)-1, 2):
        sentence = parts[i].strip()
        punct = parts[i+1]
        full_sentence = sentence + punct
        sentences.append(full_sentence)

    result = []
    for s in sentences:
        if not result:
            result.append(s)
        else:
            prev = result[-1]
            # 1. å®Œå…¨ç›¸åŒï¼Œè·³è¿‡
            if prev == s:
                continue
            # 2. å¦‚æœå½“å‰å¥æ˜¯å‰ä¸€å¥çš„å°¾éƒ¨é‡å¤ï¼Œä¹Ÿè·³è¿‡
            elif prev.endswith(s):
                continue
            else:
                result.append(s)

    return "".join(result)


